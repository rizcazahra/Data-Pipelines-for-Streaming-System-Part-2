# Data-Pipelines-for-Streaming-System-Part-2

This project is a data engineering and analytics project designed to demonstrate the integration of Apache Kafka, Apache Spark, and Apache Airflow for processing and analyzing data. It includes components for data generation, streaming data ingestion, and data processing.

1. Generating Purchase Events:
Use the event_producer.py script to simulate the generation of purchase events. 

2. Consuming and Processing Data with spark-submit
