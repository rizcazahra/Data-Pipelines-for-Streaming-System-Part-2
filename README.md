# Data-Pipelines-for-Streaming-System-Part-2

This project is a data engineering and analytics project designed to demonstrate the integration of Apache Kafka, Apache Spark, and Apache Airflow for processing and analyzing data. It includes components for data generation, streaming data ingestion, and data processing.

The output of this project will be on **console.**

1. Generating Purchase Events:
Use the event_producer.py script to simulate the generation of purchase events.

2. Using rizca-zahra-spark-event-consumer.py for event-consumer

3. Consuming and Processing Data with spark-submit
